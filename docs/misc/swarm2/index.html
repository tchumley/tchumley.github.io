<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title>swarm2 doc</title>
  <style type="text/css">code{white-space: pre;}</style>
  <style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootswatch/3.3.7/paper/bootstrap.min.css" type="text/css" />
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
</head>
<body>
  
  

  <div class="container">
    <div class="row">
            <div class="col-sm-12">
            <h1 id="computing-on-the-swarm2-cluster">Computing on the swarm2 cluster</h1>
            <p>This document outlines some basics of getting started with the swarm2 cluster.</p>
            <h2 id="getting-started-with-slurm">Getting started with SLURM</h2>
            <p>In what follows, we’ll discuss submitting jobs to the <a href="https://people.cs.umass.edu/~swarm/">swarm2 cluster</a> using SLURM and the <code>sbatch</code> command, as well as how to do computation on the cluster using an interactive shell session through the <code>srun</code> command. The examples are intentionally minimal and only a starting point. More details on using SLURM can be found on the <a href="https://people.cs.umass.edu/~swarm/index.php?n=Main.NewSwarmDoc">UMass swarm2 user documentation</a> and the <a href="https://slurm.schedmd.com/sbatch.html">SLURM sbatch man page</a>.</p>
            <h3 id="sbatch">sbatch</h3>
            <p>We give an example here on how to submit a job consisting of a python script. The only files needed are called <a href="test.py"><code>test.py</code></a> and <a href="run_python.slurm"><code>run_python.slurm</code></a>, which you can download and store in a directory on the cluster. Running the following code from your shell on the cluster will download the files and store them in a directory called <code>test</code></p>
            <div class="sourceCode" id="cb1"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb1-1"><a href="#cb1-1"></a><span class="fu">mkdir</span> -p test</span>
            <span id="cb1-2"><a href="#cb1-2"></a><span class="bu">cd</span> test</span>
            <span id="cb1-3"><a href="#cb1-3"></a><span class="fu">wget</span> https://www.mtholyoke.edu/~tchumley/misc/swarm2/test.py</span>
            <span id="cb1-4"><a href="#cb1-4"></a><span class="fu">wget</span> https://www.mtholyoke.edu/~tchumley/misc/swarm2/run_python.slurm</span></code></pre></div>
            <p>We now discuss the contents of each file and then discuss how to use them.</p>
            <h4 id="job-code">Job code</h4>
            <p>The following code, which is the contents of <code>test.py</code>, is a simple python script for demonstration purposes. It will print out the name of the file and any arguments given when run through the python interpreter.</p>
            <div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1"></a><span class="im">import</span> sys</span>
            <span id="cb2-2"><a href="#cb2-2"></a><span class="bu">print</span> (<span class="st">&quot;the script has the name </span><span class="sc">%s</span><span class="st">&quot;</span> <span class="op">%</span> (sys.argv[<span class="dv">0</span>]))</span>
            <span id="cb2-3"><a href="#cb2-3"></a><span class="cf">if</span> <span class="bu">len</span>(sys.argv) <span class="op">&gt;</span> <span class="dv">1</span>:</span>
            <span id="cb2-4"><a href="#cb2-4"></a>    <span class="bu">print</span> (<span class="st">&quot;the script has arguments </span><span class="sc">%s</span><span class="st">&quot;</span> <span class="op">%</span> (sys.argv[<span class="dv">1</span>:]))</span></code></pre></div>
            <p>You can try running the following on the cluster, to see some typical output.</p>
            <div class="sourceCode" id="cb3"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb3-1"><a href="#cb3-1"></a><span class="ex">python</span> test.py</span>
            <span id="cb3-2"><a href="#cb3-2"></a><span class="ex">python</span> test.py hello world</span></code></pre></div>
            <h4 id="job-submission-code">Job submission code</h4>
            <p>The following code is the contents of <code>run_python.slurm</code>.</p>
            <div class="sourceCode" id="cb4"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb4-1"><a href="#cb4-1"></a><span class="co">#!/bin/bash</span></span>
            <span id="cb4-2"><a href="#cb4-2"></a><span class="co">#SBATCH --nodes=1</span></span>
            <span id="cb4-3"><a href="#cb4-3"></a><span class="co">#SBATCH --time=1:30:00</span></span>
            <span id="cb4-4"><a href="#cb4-4"></a><span class="co">#SBATCH --out=python_job-%j.log</span></span>
            <span id="cb4-5"><a href="#cb4-5"></a></span>
            <span id="cb4-6"><a href="#cb4-6"></a><span class="co"># load python interpreter</span></span>
            <span id="cb4-7"><a href="#cb4-7"></a><span class="ex">module</span> load python/3.7.3 </span>
            <span id="cb4-8"><a href="#cb4-8"></a></span>
            <span id="cb4-9"><a href="#cb4-9"></a><span class="co"># run the python script, given as command line argument</span></span>
            <span id="cb4-10"><a href="#cb4-10"></a><span class="ex">python</span> <span class="va">$1</span></span></code></pre></div>
            <p>The code contains three parts:</p>
            <ul>
            <li><p>Set up of some SLURM options:</p>
            <ul>
            <li><code>#SBATCH --nodes=1</code> asks for one node on the cluster</li>
            <li><code>#SBATCH --time=1:30:00</code> asks to reserve the node for 1.5 hours</li>
            <li><code>#SBATCH --out=python_job-%j.log</code> asks to print any outputs of the job to a log file with the specified name format. The <code>%j</code> part of the file name will give the job number assigned by the scheduler.</li>
            </ul></li>
            <li><p>Add necessary modules to your environment. Here we just add the module for python, version 3.7.3.</p></li>
            <li><p>Run the desired commands for the job. Here <code>$1</code> corresponds to the first argument that will be fed into <code>run_python.slurm</code> when we submit our job.</p></li>
            </ul>
            <h4 id="submitting-the-job">Submitting the job</h4>
            <p>We are now ready to submit some jobs. Run the following commands to submit a couple jobs. Recall that you can see the results of the jobs in two files of the form <code>python_job-%j.log</code>.</p>
            <div class="sourceCode" id="cb5"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb5-1"><a href="#cb5-1"></a><span class="ex">sbatch</span> run_python.slurm test.py</span>
            <span id="cb5-2"><a href="#cb5-2"></a><span class="ex">sbatch</span> run_python.slurm <span class="st">&quot;test.py hello world&quot;</span></span></code></pre></div>
            <h3 id="srun">srun</h3>
            <p>For simple <em>one-liner</em> jobs, it’s possible to use the <code>srun</code> command instead of going through the trouble of making a slurm file and using <code>sbatch</code>. Running the following will submit a job to a node on the cluster and display the results directly to your shell instead of writing a log file.</p>
            <div class="sourceCode" id="cb6"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb6-1"><a href="#cb6-1"></a><span class="ex">srun</span> test.py hello world</span></code></pre></div>
            <p>It’s also possible to use options with srun, as in the following.</p>
            <div class="sourceCode" id="cb7"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb7-1"><a href="#cb7-1"></a><span class="ex">srun</span> --nodes=1 --time=1:30:00 test.py hello world</span></code></pre></div>
            <p>Note that <code>srun</code> can be also used to request an interactive shell on a node so that you can run commands manually one at a time.</p>
            <div class="sourceCode" id="cb8"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb8-1"><a href="#cb8-1"></a><span class="ex">srun</span> --pty /bin/bash</span></code></pre></div>
            <p>It will appear that nothing has changed, but your session will actually be running on one of the compute nodes. Try typing <code>hostname</code> to see that you’re actually logged into one of the compute nodes. When you’re done, you can type <code>exit</code> to return to the head node.</p>
            <h2 id="some-remarks-on-parallel-computing">Some remarks on parallel computing</h2>
            <p>There are some small differences in the options needed when running jobs that depend on the parallel computing paradigm used. The two examples below show the options that should be set for running a (shared memory) multithreaded job and for running a (non-shared-memory) multiprocess job. The options will be, respectively, <code>cpus-per-task</code> and <code>ntasks-per-node</code>.</p>
            <h3 id="multithreading">Multithreading</h3>
            <p>When running a job that consists of a single process (referred to as a <code>task</code> in the language of SLURM) that can spawn multiple threads, the SLURM <code>cpus-per-task</code> option should be set in order to request the use of multiple cores on the compute node.</p>
            <p>The example code below shows a script that generates some matrices and performs matrix multiplication. Python’s <code>numpy</code> library takes advantage of multithreading by default and this is used when doing the multiplication. The code is contained in a file called <a href="multithreading_test.py"><code>multithreading_test.py</code></a>.</p>
            <div class="sourceCode" id="cb9"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
            <span id="cb9-2"><a href="#cb9-2"></a><span class="im">import</span> time</span>
            <span id="cb9-3"><a href="#cb9-3"></a></span>
            <span id="cb9-4"><a href="#cb9-4"></a>start_time <span class="op">=</span> time.time()</span>
            <span id="cb9-5"><a href="#cb9-5"></a></span>
            <span id="cb9-6"><a href="#cb9-6"></a>a <span class="op">=</span> np.random.randn(<span class="dv">5000</span>, <span class="dv">50000</span>)</span>
            <span id="cb9-7"><a href="#cb9-7"></a>b <span class="op">=</span> np.random.randn(<span class="dv">50000</span>, <span class="dv">5000</span>)</span>
            <span id="cb9-8"><a href="#cb9-8"></a>ran_time <span class="op">=</span> time.time() <span class="op">-</span> start_time</span>
            <span id="cb9-9"><a href="#cb9-9"></a><span class="bu">print</span>(<span class="st">&quot;time to complete random matrix generation was </span><span class="sc">%s</span><span class="st"> seconds&quot;</span> <span class="op">%</span> ran_time)</span>
            <span id="cb9-10"><a href="#cb9-10"></a></span>
            <span id="cb9-11"><a href="#cb9-11"></a>np.dot(a, b) <span class="co"># this line is multithreaded</span></span>
            <span id="cb9-12"><a href="#cb9-12"></a><span class="bu">print</span>(<span class="st">&quot;time to complete dot was </span><span class="sc">%s</span><span class="st"> seconds&quot;</span> <span class="op">%</span> (time.time() <span class="op">-</span> start_time <span class="op">-</span> ran_time))</span></code></pre></div>
            <p>Now try submitting the following jobs and notice the time savings incurred by requesting more cores through the <code>cpus-per-task</code> option. Note that the upper limit for the <code>cpus-per-task</code> option is the number of cores on the compute node being used.</p>
            <div class="sourceCode" id="cb10"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb10-1"><a href="#cb10-1"></a><span class="ex">sbatch</span> --cpus-per-task=5  run_python.slurm multithreading_test.py</span>
            <span id="cb10-2"><a href="#cb10-2"></a><span class="ex">sbatch</span> --cpus-per-task=10 run_python.slurm multithreading_test.py</span>
            <span id="cb10-3"><a href="#cb10-3"></a><span class="ex">sbatch</span> --cpus-per-task=20 run_python.slurm multithreading_test.py</span></code></pre></div>
            <h3 id="multiprocessing">Multiprocessing</h3>
            <p>When running a job that consists of a process that can spawn multiple, memory independent processes, the SLURM <code>ntasks-per-node</code> option should be set in order to request the use of multiple cores on the compute node.</p>
            <p>The example code below shows a script that performs a Monte Carlo simulation to estimate the value of pi. Each core will independently perform a fraction of the computations to be done by using Python’s <code>multiprocessing</code> library and the <code>Pool</code> and <code>map</code> functionalities. The code is contained in a file called <a href="multiprocessing_test.py"><code>multiprocessing_test.py</code></a>.</p>
            <div class="sourceCode" id="cb11"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1"></a><span class="im">import</span> random</span>
            <span id="cb11-2"><a href="#cb11-2"></a><span class="im">import</span> time</span>
            <span id="cb11-3"><a href="#cb11-3"></a><span class="im">import</span> os</span>
            <span id="cb11-4"><a href="#cb11-4"></a><span class="im">from</span> multiprocessing <span class="im">import</span> Pool</span>
            <span id="cb11-5"><a href="#cb11-5"></a></span>
            <span id="cb11-6"><a href="#cb11-6"></a></span>
            <span id="cb11-7"><a href="#cb11-7"></a><span class="co"># count the number of random points </span></span>
            <span id="cb11-8"><a href="#cb11-8"></a><span class="co"># that fall in the unit circle</span></span>
            <span id="cb11-9"><a href="#cb11-9"></a><span class="co"># out of n points</span></span>
            <span id="cb11-10"><a href="#cb11-10"></a><span class="kw">def</span> monte_carlo_steps(n):   </span>
            <span id="cb11-11"><a href="#cb11-11"></a>    count <span class="op">=</span> <span class="dv">0</span></span>
            <span id="cb11-12"><a href="#cb11-12"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n):</span>
            <span id="cb11-13"><a href="#cb11-13"></a>        x <span class="op">=</span> random.random()</span>
            <span id="cb11-14"><a href="#cb11-14"></a>        y <span class="op">=</span> random.random()</span>
            <span id="cb11-15"><a href="#cb11-15"></a>                </span>
            <span id="cb11-16"><a href="#cb11-16"></a>        <span class="cf">if</span> x<span class="op">*</span>x <span class="op">+</span> y<span class="op">*</span>y <span class="op">&lt;=</span> <span class="dv">1</span>:</span>
            <span id="cb11-17"><a href="#cb11-17"></a>            count<span class="op">=</span>count<span class="op">+</span><span class="dv">1</span>       </span>
            <span id="cb11-18"><a href="#cb11-18"></a>    </span>
            <span id="cb11-19"><a href="#cb11-19"></a>    <span class="cf">return</span> count</span>
            <span id="cb11-20"><a href="#cb11-20"></a></span>
            <span id="cb11-21"><a href="#cb11-21"></a></span>
            <span id="cb11-22"><a href="#cb11-22"></a><span class="cf">if</span> <span class="va">__name__</span><span class="op">==</span><span class="st">&#39;__main__&#39;</span>:</span>
            <span id="cb11-23"><a href="#cb11-23"></a>    start_time <span class="op">=</span> time.time()</span>
            <span id="cb11-24"><a href="#cb11-24"></a>    np <span class="op">=</span> <span class="bu">int</span>(os.environ[<span class="st">&#39;SLURM_NTASKS_PER_NODE&#39;</span>])</span>
            <span id="cb11-25"><a href="#cb11-25"></a>    <span class="bu">print</span>(<span class="st">&quot;Number of cores allocated by slurm: &quot;</span>, np) </span>
            <span id="cb11-26"><a href="#cb11-26"></a></span>
            <span id="cb11-27"><a href="#cb11-27"></a>    <span class="co"># Nummber of points to use for the pi estimation</span></span>
            <span id="cb11-28"><a href="#cb11-28"></a>    n <span class="op">=</span> <span class="dv">10000000</span></span>
            <span id="cb11-29"><a href="#cb11-29"></a>    </span>
            <span id="cb11-30"><a href="#cb11-30"></a>    <span class="co"># each worker process gets floor(n/np) number of points to calculate pi from</span></span>
            <span id="cb11-31"><a href="#cb11-31"></a>    part_count <span class="op">=</span> [n <span class="op">//</span> np <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(np)]</span>
            <span id="cb11-32"><a href="#cb11-32"></a></span>
            <span id="cb11-33"><a href="#cb11-33"></a>    <span class="co">#Create the worker pool</span></span>
            <span id="cb11-34"><a href="#cb11-34"></a>    pool <span class="op">=</span> Pool(processes <span class="op">=</span> np)   </span>
            <span id="cb11-35"><a href="#cb11-35"></a></span>
            <span id="cb11-36"><a href="#cb11-36"></a>    <span class="co"># parallel map</span></span>
            <span id="cb11-37"><a href="#cb11-37"></a>    count <span class="op">=</span> pool.<span class="bu">map</span>(monte_carlo_steps, part_count)</span>
            <span id="cb11-38"><a href="#cb11-38"></a></span>
            <span id="cb11-39"><a href="#cb11-39"></a>    <span class="bu">print</span>(<span class="st">&quot;Esitmated value of pi: &quot;</span>, <span class="bu">sum</span>(count)<span class="op">/</span>(n<span class="op">*</span><span class="fl">1.0</span>)<span class="op">*</span><span class="dv">4</span>)</span>
            <span id="cb11-40"><a href="#cb11-40"></a>    <span class="bu">print</span>(<span class="st">&quot;Time to complete estimation was </span><span class="sc">%s</span><span class="st"> seconds&quot;</span> <span class="op">%</span> (time.time() <span class="op">-</span> start_time))</span></code></pre></div>
            <p>Try submitting the following jobs and notice the time savings incurred by requesting more cores through the <code>ntasks-per-node</code> option. Note that the upper limit for the <code>ntasks-per-node</code> option is the number of cores on the compute node being used.</p>
            <div class="sourceCode" id="cb12"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb12-1"><a href="#cb12-1"></a><span class="ex">sbatch</span> --ntasks-per-node=5  run_python.slurm multiprocessing_test.py</span>
            <span id="cb12-2"><a href="#cb12-2"></a><span class="ex">sbatch</span> --ntasks-per-node=10 run_python.slurm multiprocessing_test.py</span>
            <span id="cb12-3"><a href="#cb12-3"></a><span class="ex">sbatch</span> --ntasks-per-node=20 run_python.slurm multiprocessing_test.py</span></code></pre></div>
            <h2 id="miscellaneous-tips">Miscellaneous tips</h2>
            <h3 id="accessing-storage-directories">accessing storage directories</h3>
            <p>Your account on the cluster includes access to the <code>work1</code> and <code>scratch1</code> storage space. This can be important because your home directory is limited to 10GB of space, while <code>work1</code> has a 2TB quota and <code>scratch1</code> has no quota at all. Note that <code>work1</code> is backed up nightly but <code>scratch1</code> is never backed up. Your space is located in the following directories</p>
            <ul>
            <li><code>/mnt/nfs/work1</code></li>
            <li><code>/mnt/nfs/scratch1</code></li>
            </ul>
            <p>However, it’s possible to link to them from your home directory for easier access by running the following commands. Make sure <code>username</code> is replaced with your actual username.</p>
            <div class="sourceCode" id="cb13"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb13-1"><a href="#cb13-1"></a><span class="fu">ln</span> -s /mnt/nfs/work1/username ~/work1</span>
            <span id="cb13-2"><a href="#cb13-2"></a><span class="fu">ln</span> -s /mnt/nfs/scratch1/username ~/scratch1</span></code></pre></div>
            <p>By the way, if you ever want to know how much space is being used by a given directory, you can run <code>du -sh</code>.</p>
            <h3 id="ssh-keys">ssh keys</h3>
            <p>It’s possible to ssh into the cluster without having to type your password in each time. Instead, you can authenticate using an ssh key. Please be aware that the following directions assume that you’re running macOS or Linux on your local computer.</p>
            <p>The first line below generates a key, and the second line copies it onto the cluster. After running this, you won’t be prompted for a password when logging in from the computer on which you generated the key. Make sure <code>username</code> is replaced with your actual username.</p>
            <div class="sourceCode" id="cb14"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb14-1"><a href="#cb14-1"></a><span class="fu">ssh-keygen</span> -t rsa</span>
            <span id="cb14-2"><a href="#cb14-2"></a><span class="fu">cat</span> ~/.ssh/id_rsa.pub <span class="kw">|</span> <span class="fu">ssh</span> username@swarm2.cs.umass.edu <span class="st">&#39;cat &gt;&gt; ~/.ssh/authorized_keys&#39;</span></span></code></pre></div>
            <h3 id="sshfs">sshfs</h3>
            <p><a href="sshfs-screenshot.png"><img src="sshfs-screenshot.png" class="img-responsive center-block" width="350" /></a></p>
            <p>It’s possible to access your files on the cluster using sshfs. The following directions, specific to macOS, will make it look your home directory on the cluster is a mounted drive, like a USB stick for example.</p>
            <ol type="1">
            <li><p>Begin by downloading and installing both <code>FUSE for macOS</code> and <code>SSHFS</code> from the <a href="https://osxfuse.github.io">FUSE</a> website.</p></li>
            <li><p>Then, running the following will create, on your local computer’s home directory, a folder called swarm2 onto which your cluster’s home directory will be remotely mounted.</p></li>
            </ol>
            <div class="sourceCode" id="cb15"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb15-1"><a href="#cb15-1"></a><span class="fu">mkdir</span> -p ~/swarm2</span>
            <span id="cb15-2"><a href="#cb15-2"></a><span class="ex">sshfs</span> swarm2.cs.umass.edu: ~/swarm2 -o reconnect -o volname=swarm2 -o IdentityFile=~/.ssh/id_rsa -o follow_symlinks</span></code></pre></div>
            <p>The cluster directory can be unmounted if necessary by running</p>
            <div class="sourceCode" id="cb16"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb16-1"><a href="#cb16-1"></a><span class="fu">umount</span> swarm2</span></code></pre></div>
            <ol start="3" type="1">
            <li>(Optional) The following commands will make a directory on your local home directory called <code>bin</code> and then save the commands above into a file called <code>mount-swarm2.sh</code></li>
            </ol>
            <div class="sourceCode" id="cb17"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb17-1"><a href="#cb17-1"></a><span class="fu">mkdir</span> -p ~/bin</span>
            <span id="cb17-2"><a href="#cb17-2"></a><span class="fu">cat</span> <span class="op">&gt;&gt;</span> ~/bin/mount-swarm2.sh <span class="op">&lt;&lt; EOF</span></span>
            <span id="cb17-3"><a href="#cb17-3"></a>#!/bin/bash</span>
            <span id="cb17-4"><a href="#cb17-4"></a>mkdir -p ~/swarm2</span>
            <span id="cb17-5"><a href="#cb17-5"></a>sshfs swarm2.cs.umass.edu: ~/swarm2 -o reconnect -o volname=swarm2 -o IdentityFile=~/.ssh/id_rsa -o follow_symlinks</span>
            <span id="cb17-6"><a href="#cb17-6"></a><span class="op">EOF</span></span>
            <span id="cb17-7"><a href="#cb17-7"></a><span class="fu">chmod</span> +x ~/mount-swarm2.sh</span></code></pre></div>
            <p>Now, whenever you want to mount your remote directory, you can run</p>
            <div class="sourceCode" id="cb18"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb18-1"><a href="#cb18-1"></a><span class="ex">~/bin/mount-swarm2.sh</span></span></code></pre></div>
      </div>
    </div>
  </div>
  
    <footer class="footer navbar-static-bottom">
     <div class="container">
     <br><br>
     <p class="text-muted small">Last modified Sun Oct 31 18:17:33 UTC 2021</p>
     </div>
  </footer>
    
    	<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js" type="text/javascript"></script>
    	<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" type="text/javascript"></script>
    
        <script>$("table:not([class~='table'])").addClass(" table table-hover table-bordered table-striped");</script>
    
  
  
</body>
</html>
